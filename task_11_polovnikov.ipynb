{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "646bebb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from stochastic.processes.diffusion import VasicekProcess\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f37f014",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "OU_SPEED = 1\n",
    "OU_MEAN = 100\n",
    "OU_VOL = 2\n",
    "\n",
    "OU_PRICE_STEP = 0.1\n",
    "OU_PRICE_MAX_DEV = 8\n",
    "\n",
    "TRAIN_HISTORY_LEN = 5_000_000\n",
    "VAL_HISTORY_LEN = 100_000\n",
    "TEST_HISTORY_LEN = 200_000\n",
    "EPISODE_LEN = 10192\n",
    "\n",
    "MAX_HOLDING = 3\n",
    "\n",
    "EXPLORATION_EPSILON = 0.1\n",
    "\n",
    "TD_N_EPOCHS = 10000\n",
    "# TD_N_EPISODES = 25 # HOMEWORK\n",
    "TD_GAMMA = 0.9 # REWARDS DISCOUNT FACTOR\n",
    "TD_LR = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24e08ed",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8117d5e",
   "metadata": {},
   "source": [
    "# Data preparation: create a format compatible with RL environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0aae00fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class OUDataProvider:\n",
    "    def __init__(self,speed,mean,vol,price_step,price_max_dev,history_len,episode_len):\n",
    "        self.speed = speed\n",
    "        self.mean = mean\n",
    "        self.vol = vol\n",
    "        self.price_step = price_step\n",
    "        self.price_max_dev = price_max_dev\n",
    "        self.history_len = history_len\n",
    "        self.episode_len = episode_len\n",
    "        \n",
    "        vp = VasicekProcess(speed=self.speed,mean=self.mean,vol=self.vol,t=episode_len)\n",
    "        self.history = pd.Series(vp.sample(self.history_len,self.mean),vp.times(self.history_len)).to_frame('price')\n",
    "        self.history['level'] = (((self.history['price']-self.mean)/self.price_step).round()).astype(int).clip(-self.price_max_dev,self.price_max_dev)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.history_len - self.episode_len + 1\n",
    "    \n",
    "    def __getitem__(self,i):\n",
    "        return self.history.iloc[i:i+self.episode_len]\n",
    "    \n",
    "    def price_to_level(self,price):\n",
    "        price_level = int(np.round((price-self.mean)/self.price_step))\n",
    "        return -self.price_max_dev if price_level < -self.price_max_dev else self.price_max_dev if price_level > self.price_max_dev else price_level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e749be",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e02869",
   "metadata": {},
   "source": [
    "# RL Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cac208e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self,max_holding,transaction_fee:float=0):\n",
    "        self.max_holding = max_holding\n",
    "        self.transaction_fee = transaction_fee\n",
    "        \n",
    "        self.possible_holdings = list(range(-self.max_holding,1+self.max_holding))\n",
    "        self.n_states = len(self.possible_holdings)\n",
    "        \n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self,holding=None):\n",
    "        self.holding = holding or 0\n",
    "        self.cash = 0.\n",
    "        self.price = None\n",
    "        \n",
    "        self.track = list() # here we put all history of agent's activity\n",
    "    \n",
    "    def value(self):\n",
    "        return self.cash + self.holding * self.price\n",
    "    \n",
    "    def holding_to_state(self,holding):\n",
    "        return holding + self.max_holding\n",
    "    \n",
    "    def state_to_holding(self,state):\n",
    "        return state - self.max_holding\n",
    "    \n",
    "    def is_action_available(self,quantity:int):\n",
    "        return self.holding + quantity <= self.max_holding and self.holding + quantity >= -self.max_holding\n",
    "    \n",
    "    def act(self,price:float,quantity:int):\n",
    "        \"\"\"\n",
    "        execute a trade by `price` in amount of `quantity`\n",
    "        \"\"\"\n",
    "        \n",
    "        holding = self.holding\n",
    "        \n",
    "        if abs(quantity):\n",
    "            transaction_cost = price * abs(quantity) * self.transaction_fee\n",
    "            current_reward = self.holding * (price-self.price) if self.price is not None else 0.\n",
    "            current_reward -= transaction_cost\n",
    "\n",
    "            self.holding += quantity\n",
    "            self.cash -= price * quantity + transaction_cost\n",
    "        else:\n",
    "            current_reward = 0.\n",
    "            \n",
    "        self.price = price # refresh agent's information about current price\n",
    "        \n",
    "        self.track.append( (holding,price,quantity,current_reward,self.value()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "285716e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Episode:\n",
    "    def __init__(self,data):\n",
    "        self.data = data\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.t = 0\n",
    "    \n",
    "    def item(self):\n",
    "        return self.data.iloc[self.t]\n",
    "    \n",
    "    def price(self):\n",
    "        return self.item().price\n",
    "    \n",
    "    def level(self):\n",
    "        return int(self.item().level)\n",
    "    \n",
    "    def finished(self):\n",
    "        return self.t + 1 >= self.data.shape[0]\n",
    "    \n",
    "    def next(self):\n",
    "        if self.finished():\n",
    "            return False\n",
    "        else:\n",
    "            self.t += 1\n",
    "            return True\n",
    "\n",
    "class Market:\n",
    "    def __init__(self,data_provider:OUDataProvider):\n",
    "        self.data_provider = data_provider\n",
    "        \n",
    "        self.possible_levels = list(range(-self.data_provider.price_max_dev,1+self.data_provider.price_max_dev))\n",
    "        self.n_states = len(self.possible_levels)\n",
    "        \n",
    "    def level_to_state(self,level):\n",
    "        return level + self.data_provider.price_max_dev\n",
    "    \n",
    "    def state_to_level(self,state):\n",
    "        return state - self.data_provider.price_max_dev\n",
    "        \n",
    "    def price_to_state(self,price):\n",
    "        return self.level_to_state(self.data_provider.price_to_level(price))\n",
    "    \n",
    "    def n_episodes(self):\n",
    "        return len(self.data_provider)\n",
    "    \n",
    "    def init_episode(self,i=None): # if i is None, then we take random episode\n",
    "        if i is None:\n",
    "            i = random.randint(0,len(self.data_provider))\n",
    "        return Episode(self.data_provider[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c82e0dec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self,market:Market,agent:Agent):\n",
    "        self.market = market\n",
    "        self.agent = agent\n",
    "        \n",
    "        self.n_price_levels = self.market.n_states\n",
    "        self.n_holding_levels = self.agent.n_states\n",
    "        self.n_states = self.n_price_levels * self.n_holding_levels\n",
    "\n",
    "        # all possible actions of agent in the existing framework ranges from\n",
    "        # selling 6 units\n",
    "        # to\n",
    "        # buying 6 units\n",
    "        # (this makes up 13 options in our example)\n",
    "        # however, at any given point not all of these actions are eligible\n",
    "        \n",
    "        self.possible_quantities = list(range(-2*self.agent.max_holding,1+2*self.agent.max_holding))\n",
    "        self.n_actions = len(self.possible_quantities)\n",
    "        # action indexed 0 - sell 6 units\n",
    "        # ...\n",
    "        # action indexed 12 - buy 6 units\n",
    "    \n",
    "    def state_2d_to_1d(self,price_level,holding):\n",
    "        market_state = self.market.level_to_state(price_level)\n",
    "        holding_state = self.agent.holding_to_state(holding)\n",
    "        return market_state * self.n_holding_levels + holding_state\n",
    "    \n",
    "    def state_1d_to_2d(self,state):\n",
    "        market_state = state // self.n_holding_levels\n",
    "        price_level = self.market.state_to_level(market_state)\n",
    "        \n",
    "        holding_state = state % self.n_holding_levels\n",
    "        holding = self.agent.state_to_holding(holding_state)\n",
    "        \n",
    "        return (price_level,holding)\n",
    "    \n",
    "    def quantity_to_action(self,quantity):\n",
    "        return quantity + 2*self.agent.max_holding\n",
    "    \n",
    "    def action_to_quantity(self,action):\n",
    "        return action - 2*self.agent.max_holding\n",
    "    \n",
    "    def eligible_actions(self,state):\n",
    "        price_level,holding = self.state_1d_to_2d(state)\n",
    "        min_quantity = -self.agent.max_holding - holding\n",
    "        max_quantity = self.agent.max_holding - holding\n",
    "        return [self.quantity_to_action(quantity) for quantity in range(min_quantity,1+max_quantity)]\n",
    "    \n",
    "    def generate_Q(self):\n",
    "        Q = np.array([np.isin(np.arange(self.n_actions),self.eligible_actions(state)).astype(float) for state in range(self.n_states)])\n",
    "        return np.where(Q==1,0,np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7a25169",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Policy:\n",
    "    def __init__(self,model,Q):\n",
    "        self.model = model\n",
    "        self.Q = Q\n",
    "    \n",
    "    def random_action(self,state):\n",
    "        eligible_actions = np.where(np.isfinite(self.Q[state]))[0]\n",
    "        return random.choice(eligible_actions)\n",
    "    \n",
    "    def best_action(self,state):\n",
    "        q = self.Q[state]\n",
    "        eligible_actions = np.where(np.isfinite(q))[0]\n",
    "        q = q[eligible_actions]\n",
    "        argmax = np.where(q==q.max())[0]\n",
    "        return eligible_actions[random.choice(argmax)]\n",
    "    \n",
    "    def epsilon_greedy_action(self,state,epsilon):\n",
    "        if random.uniform(0,1) < epsilon:\n",
    "            return self.random_action(state)\n",
    "        else:\n",
    "            return self.best_action(state)\n",
    "    \n",
    "    def evaluate(self,episode): # evaluate episode\n",
    "        self.model.agent.reset()\n",
    "        episode.reset()\n",
    "        \n",
    "        while not episode.finished():\n",
    "            state = self.model.state_2d_to_1d(episode.level(),self.model.agent.holding) # read current state of model\n",
    "            action = self.best_action(state) # determine best action from given model state\n",
    "            quantity = self.model.action_to_quantity(action) # determine amount of asset we need to execute on the market\n",
    "            self.model.agent.act(episode.price(),quantity) # send order\n",
    "            \n",
    "            episode.next()\n",
    "        \n",
    "        return pd.DataFrame(self.model.agent.track,columns=['holding','price','quantity','reward','value'])\n",
    "    \n",
    "    def run(self,episode,epsilon,holding=None): # episode run\n",
    "        states,actions = list(),list()\n",
    "        \n",
    "        self.model.agent.reset(holding or random.randint(-self.model.agent.max_holding,self.model.agent.max_holding))\n",
    "        episode.reset()\n",
    "        while not episode.finished():\n",
    "            state = self.model.state_2d_to_1d(episode.level(),self.model.agent.holding) # read current state of model\n",
    "            states.append(state)\n",
    "            \n",
    "            action = self.epsilon_greedy_action(state,epsilon)\n",
    "            actions.append(action)\n",
    "            \n",
    "            quantity = self.model.action_to_quantity(action)\n",
    "            self.model.agent.act(episode.price(),quantity)\n",
    "            \n",
    "            episode.next()\n",
    "        \n",
    "        rewards = [item[3] for item in self.model.agent.track]\n",
    "        \n",
    "        return pd.DataFrame({\n",
    "            'state' : states,\n",
    "            'action' : actions,\n",
    "            'reward' : rewards\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "770ee225",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a101b12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data = OUDataProvider(\n",
    "    speed = OU_SPEED,\n",
    "    mean = OU_MEAN,\n",
    "    vol = OU_VOL,\n",
    "    price_step = OU_PRICE_STEP,\n",
    "    price_max_dev = OU_PRICE_MAX_DEV,\n",
    "    history_len = TRAIN_HISTORY_LEN,\n",
    "    episode_len = EPISODE_LEN,\n",
    ")\n",
    "\n",
    "val_data = OUDataProvider(\n",
    "    speed = OU_SPEED,\n",
    "    mean = OU_MEAN,\n",
    "    vol = OU_VOL,\n",
    "    price_step = OU_PRICE_STEP,\n",
    "    price_max_dev = OU_PRICE_MAX_DEV,\n",
    "    history_len = VAL_HISTORY_LEN,\n",
    "    episode_len = VAL_HISTORY_LEN,\n",
    ")\n",
    "\n",
    "test_data = OUDataProvider(\n",
    "    speed = OU_SPEED,\n",
    "    mean = OU_MEAN,\n",
    "    vol = OU_VOL,\n",
    "    price_step = OU_PRICE_STEP,\n",
    "    price_max_dev = OU_PRICE_MAX_DEV,\n",
    "    history_len = TEST_HISTORY_LEN,\n",
    "    episode_len = TEST_HISTORY_LEN,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a6dc84b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "agent = Agent(MAX_HOLDING,transaction_fee=10e-4)\n",
    "\n",
    "test_model = Model(market=Market(test_data),agent=agent)\n",
    "test_P = Policy(test_model,Q)\n",
    "train_model = Model(market=Market(train_data),agent=agent)\n",
    "val_model = Model(market=Market(val_data),agent=agent)\n",
    "test_model = Model(market=Market(test_data),agent=agent)\n",
    "\n",
    "val_episode = val_model.market.init_episode(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfac6dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Situation to consider\n",
    "\n",
    "initial_state -> (action) -> new_state => here we have 1-step reward (in our case it is pnl for one time unit)\n",
    "\n",
    "VALUE(inital_state) needs to be recalculated in the following way:\n",
    "\n",
    "VALUE(initial_state) := VALUE(initial_state) + lr * (REWARD(new_state) + gamma * VALUE(new_state) - VALUE(initial_state))\n",
    "by opening brackets we obtain\n",
    "VALUE(initial_state) := VALUE(initial_state) * (1-lr) + lr * (REWARD(new_state) + gamma * VALUE(new_state))\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1847c179",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Q = train_model.generate_Q()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4e887a0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_P = Policy(train_model,Q)\n",
    "val_P = Policy(val_model,Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6302eb19",
   "metadata": {},
   "source": [
    "Let's do one iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "18a2d08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# our goal as for now to collect history of agent activity in the following way\n",
    "# (state,action,new_state,reward associated with this action)\n",
    "\n",
    "\"\"\"\n",
    "in our concrete example it should be the following list:\n",
    "[\n",
    "    (5,2,1,-0.375),\n",
    "    (1,9,4,-0.188),\n",
    "    (4,3,1,-0.396),\n",
    "    (1,5,0,-0.358)\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "activity = list(zip(run['state'],run['action'],run.shift(-1)['state'].fillna(0).astype(int),run.shift(-1)['reward']))[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7df554c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "097f2c76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nexample : for the first item in the activity:\\n(5, 2, 1, -0.37457722969903673)\\n\\nQ[5,2] = (1-lr) * Q[5,2] + lr * (-0.375 + gamma * max(Q[1]))\\n'"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# VALUE(initial_state) := VALUE(initial_state) * (1-lr) + lr * (REWARD(new_state) + gamma * VALUE(new_state))\n",
    "\n",
    "\"\"\"\n",
    "example : for the first item in the activity:\n",
    "(5, 2, 1, -0.37457722969903673)\n",
    "\n",
    "Q[5,2] = (1-lr) * Q[5,2] + lr * (-0.375 + gamma * max(Q[1]))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "4498d2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment from Chingis:\n",
    "# We may consider running this snipped several times (fixed number) so that we make a deeper propagation\n",
    "# But there is no sense to run it until Q stabilizes, because by doing this we overfit for a single episode, and after switching episode we need to recalibrate Q again\n",
    "\n",
    "for state_before,action,state_after,reward in activity:\n",
    "    Q[state_before,action] = (1-TD_LR) * Q[state_before,action] + TD_LR * (reward + TD_GAMMA * np.nanmax(Q[state_after]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b32bcc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# This comes from Monte-Carlo algorithm, modify it for TD\n",
    "\n",
    "MAX_HOLDING = 3\n",
    "EXPLORATION_EPSILON = 0.8\n",
    "TD_N_EPOCHS = 100\n",
    "TD_N_EPISODES = 25 # HOMEWORK\n",
    "TD_GAMMA = 0.9 # REWARDS DISCOUNT FACTOR\n",
    "TD_LR = 0.01\n",
    "\n",
    "# V = dict()\n",
    "\n",
    "val_pnl_history = list()\n",
    "for epoch in range(TD_N_EPOCHS):\n",
    "    # calculate value of each (state,action) cell\n",
    "    # first visit algorithm\n",
    "    \n",
    "    # a) <==================================#\n",
    "    train_episodes = list()\n",
    "    for i in range(TD_N_EPISODES):\n",
    "        train_episode = train_model.market.init_episode()\n",
    "        train_episodes.append(train_episode)\n",
    "        \n",
    "        run = train_P.run(train_episode,EXPLORATION_EPSILON/(i/10 + 1))\n",
    "        #========================#\n",
    "        \n",
    "        activity = list(zip(run['state'],run['action'],run.shift(-1)['state'].fillna(0).astype(int),run.shift(-1)['reward']))[:-1]\n",
    "        for state_before, action, state_after, reward in activity:\n",
    "            Q[state_before, action] = (1-TD_LR) * Q[state_before,action] + TD_LR * (reward + TD_GAMMA * np.nanmax(Q[state_after]))\n",
    "            \n",
    "        #========================#\n",
    "        \n",
    "#         for item in run[['state','action']].drop_duplicates().to_numpy():\n",
    "#             if tuple(item) not in V:\n",
    "#                 V[tuple(item)] = list()\n",
    "            \n",
    "#             t = run[(run.state==item[0])&(run.action==item[1])].iloc[0].name\n",
    "#             r = run.loc[t:].iloc[1:].reward.values\n",
    "#             f = np.power(MC_GAMMA,np.arange(0,r.size))\n",
    "            \n",
    "#             V[tuple(item)].append( (r*f).sum() )\n",
    "        \n",
    "#     # refresh values of Q\n",
    "#     for (state,action),vals in V.items():\n",
    "#         Q[state,action] = np.mean(vals)\n",
    "    \n",
    "    best_total_reward = None\n",
    "    best_evaluation = None\n",
    "    for train_episode in train_episodes:\n",
    "        train_evaluation = train_P.evaluate(train_episode)\n",
    "        current_total_reward = train_evaluation['reward'].sum()\n",
    "        if best_total_reward is None or current_total_reward > best_total_reward:\n",
    "            best_total_reward = current_total_reward\n",
    "            best_evaluation = train_evaluation\n",
    "    \n",
    "    val_evaluation = val_P.evaluate(val_episode)\n",
    "    \n",
    "    val_pnl_history.append(val_evaluation['reward'].sum())\n",
    "    \n",
    "    # plot\n",
    "    clear_output()\n",
    "    \n",
    "    fix,ax = plt.subplots(2,2,figsize=(25,8))\n",
    "    \n",
    "    ax[0,0].plot(best_evaluation['reward'].cumsum())\n",
    "    ax[0,0].set_title('Best training episode')\n",
    "    \n",
    "    ax[0,1].plot(val_evaluation['reward'].cumsum())\n",
    "    ax[0,1].set_title('Validation set')\n",
    "    \n",
    "    ax[1,0].plot(val_pnl_history)\n",
    "    ax[1,0].set_title('Validation PNL history')\n",
    "    ax[1,0].set_xlabel('Epoch #')\n",
    "    ax[1,0].set_ylabel('PNL')\n",
    "    \n",
    "    sns.heatmap(Q.T,ax=ax[1,1])\n",
    "    ax[1,1].invert_yaxis()\n",
    "    ax[1,1].set_title('Policy')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "96d2841f-cc9b-4266-ba3f-5e9210c5446a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f73d564bd00>]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# d)\n",
    "test_model = Model(market=Market(test_data),agent=agent)\n",
    "test_P = Policy(test_model,Q)\n",
    "test_episode = test_model.market.init_episode(0)\n",
    "test_evaluation = test_P.evaluate(test_episode)\n",
    "\n",
    "plt.plot(np.arange(test_data.episode_len-1)[:100], test_evaluation['reward'].cumsum()[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd533e71-8ade-49ce-9b11-26f3fc9c7605",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# c) compare with MC-method\n",
    "Q2 = train_model.generate_Q()\n",
    "\n",
    "train_P = Policy(train_model,Q2)\n",
    "val_P = Policy(val_model,Q2)\n",
    "test_P = Policy(test_model,Q2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12287e4c-6d86-4f8f-b836-987be8228ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# please check the time required for algorithms to perform \n",
    "# specified EPOCH and EPISODES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f88d723-702f-49fb-8035-82b7cc26f63a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "EXPLORATION_EPSILON = 0.8 # FOR b)\n",
    "\n",
    "V = dict()\n",
    "\n",
    "val_pnl_history = list()\n",
    "minimal_improvement = 0\n",
    "\n",
    "for epoch in range(TD_N_EPOCHS):\n",
    "    # calculate value of each (state,action) cell\n",
    "    # first visit algorithm\n",
    "    \n",
    "    train_episodes = list()\n",
    "    for i in range(TD_N_EPISODES): \n",
    "        train_episode = train_model.market.init_episode()\n",
    "        train_episodes.append(train_episode)\n",
    "        run = train_P.run(train_episode,EXPLORATION_EPSILON/(i/10+1)) # <= b)\n",
    "        \n",
    "        #==========#\n",
    "        #  a)\n",
    "        #==========#\n",
    "        # change first visit => every visit\n",
    "        pairs_storage = pd.DataFrame( [zip(run['state'], run['action'])] ).T\n",
    "        for item in pd.unique(pairs_storage.iloc[:,0]): #to_numpy():\n",
    "            item_count = pairs_storage[pairs_storage.iloc[:,0] == item].count()[0]\n",
    "            #print(f'item = {item}, count = {item_count}')\n",
    "            \n",
    "            t = run[(run.state==item[0])&(run.action==item[1])].index\n",
    "            #print(f't = {t}')\n",
    "\n",
    "            total_value = 0\n",
    "            for t_step in t:\n",
    "                r = run.loc[t_step:].iloc[1:].reward.values\n",
    "                f = np.power(MC_GAMMA,np.arange(0,r.size))\n",
    "                total_value += (r * f).sum()\n",
    "            total_value /= item_count\n",
    "            #print(total_value)\n",
    "            #================#\n",
    "                \n",
    "            if tuple(item) not in V:\n",
    "                V[tuple(item)] = list()\n",
    "\n",
    "            V[tuple(item)].append(total_value)\n",
    "\n",
    "    # refresh values of Q\n",
    "    for (state,action),vals in V.items():\n",
    "        Q2[state,action] = np.mean(vals)\n",
    "    \n",
    "    best_total_reward = None\n",
    "    best_evaluation = None\n",
    "    for train_episode in train_episodes:\n",
    "        train_evaluation = train_P.evaluate(train_episode)\n",
    "        current_total_reward = train_evaluation['reward'].sum()\n",
    "        if best_total_reward is None or current_total_reward > best_total_reward:\n",
    "            best_total_reward = current_total_reward\n",
    "            best_evaluation = train_evaluation\n",
    "    \n",
    "    val_evaluation = val_P.evaluate(val_episode)\n",
    "    \n",
    "    val_pnl_history.append(val_evaluation['reward'].sum())\n",
    "    \n",
    "    # c) criterion of stopping\n",
    "    # if improvement is less than minimal_improvement / 3 => stop learning\n",
    "    if epoch > 1:\n",
    "        if abs( val_pnl_history[-1] - val_pnl_history[-2] ) < minimal_improvement / 3:\n",
    "            break\n",
    "        minimal_improvement = min(minimal_improvement, abs(val_pnl_history[-1] - val_pnl_history[-2]))\n",
    "    \n",
    "    # plot\n",
    "    clear_output()\n",
    "    \n",
    "    fix,ax = plt.subplots(2,2,figsize=(25,8))\n",
    "    \n",
    "    ax[0,0].plot(best_evaluation['reward'].cumsum())\n",
    "    ax[0,0].set_title('Best training episode')\n",
    "    \n",
    "    ax[0,1].plot(val_evaluation['reward'].cumsum())\n",
    "    ax[0,1].set_title('Validation set')\n",
    "    \n",
    "    ax[1,0].plot(val_pnl_history)\n",
    "    ax[1,0].set_title('Validation PNL history')\n",
    "    ax[1,0].set_xlabel('Epoch #')\n",
    "    ax[1,0].set_ylabel('PNL')\n",
    "    \n",
    "    sns.heatmap(Q2.T,ax=ax[1,1])\n",
    "    ax[1,1].invert_yaxis()\n",
    "    ax[1,1].set_title('Policy')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0d9016-dd75-4b2d-a4ab-7c39c2d12591",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# b)\n",
    "test_model = Model(market=Market(test_data),agent=agent)\n",
    "test_P = Policy(test_model,Q2)\n",
    "test_episode = test_model.market.init_episode(0)\n",
    "test_evaluation = test_P.evaluate(test_episode)\n",
    "\n",
    "plt.plot(np.arange(test_data.episode_len-1)[:100], test_evaluation['reward'].cumsum()[:100])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
